# GPU Server 配置
MANAGEMENT_API_HOST=0.0.0.0
MANAGEMENT_API_PORT=9000

WEBSOCKET_HOST=0.0.0.0
WEBSOCKET_PORT=9001

# WebSocket 公网 URL（用于返回给客户端）
# 开发环境: ws://localhost:9001
# 生产环境: ws://your-gpu-server-ip:9001
WEBSOCKET_URL=ws://localhost:9001

# GPU 配置
CUDA_VISIBLE_DEVICES=0

# 会话配置
MAX_SESSIONS=10
SESSION_TIMEOUT_SECONDS=3600

# LLM 配置
# Ollama 服务地址
OLLAMA_BASE_URL=http://127.0.0.1:11434

# 默认 LLM 模型（Ollama 模型名称）
DEFAULT_LLM_MODEL=mistral-nemo:12b-instruct-2407-fp16

# LLM 温度参数（0.0-1.0，控制输出随机性）
LLM_TEMPERATURE=0.4

# 是否启用 LLM（如果为 false，则使用 Mock 模式）
ENABLE_LLM=true

# 按 tutor_id 配置不同模型（可选）
# TUTOR_1_LLM_MODEL=mistral-nemo:12b-instruct-2407-fp16
# TUTOR_2_LLM_MODEL=llama3.1:8b-instruct-q4_K_M
