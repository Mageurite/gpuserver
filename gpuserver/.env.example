# GPU Server 配置
MANAGEMENT_API_HOST=0.0.0.0
MANAGEMENT_API_PORT=9000

WEBSOCKET_HOST=0.0.0.0
WEBSOCKET_PORT=9001

# WebSocket 公网 URL（用于返回给客户端）
# 开发环境: ws://localhost:9001
# 生产环境: ws://your-gpu-server-ip:9001
WEBSOCKET_URL=ws://localhost:9001

# GPU 配置
CUDA_VISIBLE_DEVICES=0

# 会话配置
MAX_SESSIONS=10
SESSION_TIMEOUT_SECONDS=3600

# LLM 配置
# Ollama 服务地址
OLLAMA_BASE_URL=http://127.0.0.1:11434

# 默认 LLM 模型（Ollama 模型名称）
DEFAULT_LLM_MODEL=mistral-nemo:12b-instruct-2407-fp16

# LLM 温度参数（0.0-1.0，控制输出随机性）
LLM_TEMPERATURE=0.4

# 是否启用 LLM（如果为 false，则使用 Mock 模式）
ENABLE_LLM=true

# 按 tutor_id 配置不同模型（可选）
# TUTOR_1_LLM_MODEL=mistral-nemo:12b-instruct-2407-fp16
# TUTOR_2_LLM_MODEL=llama3.1:8b-instruct-q4_K_M

# ASR 配置
# Whisper 模型: tiny, base, small, medium, large
ASR_MODEL=base

# 是否启用 ASR（如果为 false，则使用 Mock 模式）
ENABLE_ASR=true

# ASR 设备: cuda 或 cpu
ASR_DEVICE=cuda

# ASR 默认语言: zh (中文), en (英文)
ASR_LANGUAGE=zh

# TTS 配置
# Edge TTS 声音
# 中文女声: zh-CN-XiaoxiaoNeural, zh-CN-XiaoyiNeural
# 中文男声: zh-CN-YunjianNeural, zh-CN-YunxiNeural
# 英文女声: en-US-JennyNeural, en-US-AriaNeural
# 英文男声: en-US-GuyNeural, en-US-ChristopherNeural
TTS_VOICE=zh-CN-XiaoxiaoNeural

# 是否启用 TTS（如果为 false，则使用 Mock 模式）
ENABLE_TTS=true

# RAG 配置
# 是否启用 RAG（如果为 false，则使用 Mock 模式）
ENABLE_RAG=false

# RAG 服务 URL（如果使用独立的 RAG 服务，如 http://localhost:9090）
# 留空则使用内置的 RAG 引擎
RAG_URL=

# RAG 检索返回的文档数量
RAG_TOP_K=5

# MuseTalk / Avatar 配置
# 是否启用真实 MuseTalk（如果为 false，则使用 Mock 模式）
ENABLE_AVATAR=false

# Avatar 存储目录
AVATARS_DIR=/workspace/gpuserver/data/avatars

# MuseTalk 基础目录
MUSETALK_BASE=/workspace/MuseTalk

# MuseTalk Conda 环境路径
# 生产环境设置为: /workspace/conda_envs/mt
MUSETALK_CONDA_ENV=/workspace/conda_envs/mt

# FFmpeg 路径
# 生产环境设置为: /workspace/MuseTalk/ffmpeg-master-latest-linux64-gpl/bin/ffmpeg
FFMPEG_PATH=/workspace/MuseTalk/ffmpeg-master-latest-linux64-gpl/bin/ffmpeg
