"""
MuseTalk å®æ—¶æ¨ç†å¼•æ“ - å®Œå…¨åŸºäº virtual-tutor çš„å®ç°

å‚è€ƒ: /workspace/virtual-tutor/lip-sync/musereal.py

å…³é”®æ¶æ„:
- ä½¿ç”¨ threading.Threadï¼ˆä¸æ˜¯ multiprocessing.Processï¼‰
- åœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹
- é€šè¿‡ Queue ä¼ é€’éŸ³é¢‘ç‰¹å¾å’Œè§†é¢‘å¸§
"""

import logging
import sys
import os
import time
from typing import Optional, AsyncIterator
import asyncio
from queue import Queue, Empty
from threading import Thread, Event
import pickle
from pathlib import Path

import torch
import numpy as np
import cv2

# æ·»åŠ  MuseTalk è·¯å¾„åˆ° sys.pathï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
MUSETALK_BASE = os.getenv('MUSETALK_BASE', '/workspace/MuseTalk')
if MUSETALK_BASE not in sys.path:
    sys.path.insert(0, MUSETALK_BASE)

logger = logging.getLogger(__name__)


def mirror_index(size: int, index: int) -> int:
    """é•œåƒç´¢å¼•"""
    turn = index // size
    res = index % size
    return res if turn % 2 == 0 else size - res - 1


@torch.no_grad()
def inference_loop(
    render_event: Event,
    batch_size: int,
    input_latent_list_cycle,
    coord_list_cycle,
    frame_list_cycle,
    mask_list_cycle,
    mask_coords_list_cycle,
    audio_feat_queue: Queue,
    res_frame_queue: Queue,
    vae, unet, pe, timesteps
):
    """
    æ¨ç†å¾ªç¯ï¼ˆåœ¨ Thread ä¸­è¿è¡Œï¼‰

    å‚è€ƒ: virtual-tutor/lip-sync/musereal.py inference()
    """
    from musetalk.utils.blending import get_image_blending

    length = len(coord_list_cycle)
    index = 0
    count = 0
    counttime = 0

    logger.info('Inference thread started')

    while render_event.is_set():
        try:
            # ä»é˜Ÿåˆ—è·å–éŸ³é¢‘ç‰¹å¾ï¼ˆ1ç§’è¶…æ—¶ï¼‰
            whisper_chunks = audio_feat_queue.get(timeout=1)
        except Empty:
            # é˜Ÿåˆ—ä¸ºç©ºï¼ŒçŸ­æš‚ä¼‘çœ é¿å… CPU 100%
            time.sleep(0.1)
            continue

        starttime = time.perf_counter()

        # æ‰¹é‡æ¨ç†
        whisper_batch = np.stack(whisper_chunks)
        latent_batch = []
        for i in range(batch_size):
            idx = mirror_index(length, index + i)
            latent = input_latent_list_cycle[idx]
            latent_batch.append(latent)

        latent_batch = torch.cat(latent_batch, dim=0)

        # å‡†å¤‡éŸ³é¢‘ç‰¹å¾
        audio_feature_batch = torch.from_numpy(whisper_batch)
        audio_feature_batch = audio_feature_batch.to(
            device=unet.device,
            dtype=unet.model.dtype
        )
        audio_feature_batch = pe(audio_feature_batch)
        latent_batch = latent_batch.to(dtype=unet.model.dtype)

        # UNet æ¨ç†
        pred_latents = unet.model(
            latent_batch,
            timesteps,
            encoder_hidden_states=audio_feature_batch
        ).sample

        # VAE è§£ç 
        recon = vae.decode_latents(pred_latents)

        # è°ƒè¯•: æ£€æŸ¥ recon çš„ç±»å‹å’Œå½¢çŠ¶
        logger.info(f"recon type: {type(recon)}, shape: {recon.shape if hasattr(recon, 'shape') else 'N/A'}")

        elapsed = time.perf_counter() - starttime
        counttime += elapsed
        count += batch_size

        if count >= 100:
            logger.info(f"Avg infer FPS: {count/counttime:.2f}")
            count = 0
            counttime = 0

        # æ··åˆç”Ÿæˆçš„å¸§
        for i, res_frame in enumerate(recon):
            idx = mirror_index(length, index)

            # è°ƒè¯•: æ£€æŸ¥ res_frame çš„ç±»å‹å’Œå½¢çŠ¶
            if i == 0:  # åªæ‰“å°ç¬¬ä¸€å¸§çš„ä¿¡æ¯
                logger.info(f"res_frame[{i}] type: {type(res_frame)}, shape: {res_frame.shape if hasattr(res_frame, 'shape') else 'N/A'}")
                logger.info(f"res_frame[{i}] dtype: {res_frame.dtype}, min: {res_frame.min()}, max: {res_frame.max()}")

            # å¸§æ··åˆ
            bbox = coord_list_cycle[idx]
            ori_frame = frame_list_cycle[idx].copy()

            try:
                # VAE decode_latents å·²ç»è¿”å› uint8 BGR æ ¼å¼ï¼ˆè§ vae.py:106-107ï¼‰
                # ä¸éœ€è¦ä»»ä½•è½¬æ¢ï¼Œç›´æ¥ä½¿ç”¨
                res_frame_np = res_frame  # ç›´æ¥ä½¿ç”¨ VAE è¾“å‡º
                
                # è°ƒè¯•ï¼šæ£€æŸ¥ res_frame çš„å€¼
                if index == 0:
                    logger.info(f"[Debug] res_frame from VAE: dtype={res_frame_np.dtype}, min={res_frame_np.min()}, max={res_frame_np.max()}")

                # CRITICAL: Resize face to bbox size before blending
                # å‚è€ƒ virtual-tutor/lip-sync/musereal.py:279
                x, y, x1, y1 = bbox
                res_frame_resized = cv2.resize(res_frame_np, (x1-x, y1-y))

                # è°ƒè¯•ï¼šæ‰“å° mask ä¿¡æ¯
                if index == 0:
                    mask_for_blend = mask_list_cycle[idx]
                    logger.info(f"[Debug] mask shape: {mask_for_blend.shape}, dtype: {mask_for_blend.dtype}")
                    logger.info(f"[Debug] mask min: {mask_for_blend.min()}, max: {mask_for_blend.max()}")
                    logger.info(f"[Debug] mask_coords: {mask_coords_list_cycle[idx]}")
                    logger.info(f"[Debug] bbox: {bbox}")

                # ä½¿ç”¨ get_image_blending ä»£æ›¿ get_image
                # get_image_blending ä¸éœ€è¦ FaceParsing æ¨¡å‹
                combined_frame = get_image_blending(
                    ori_frame,
                    res_frame_resized,         # ä½¿ç”¨ resize åçš„ face
                    bbox,                      # face_box
                    mask_list_cycle[idx],      # mask_array
                    mask_coords_list_cycle[idx] # crop_box
                )
            except Exception as e:
                logger.error(f"Frame blending error: {e}")
                logger.error(f"Traceback: ", exc_info=True)
                combined_frame = ori_frame

            # ç«‹å³æ”¾å…¥å¸§é˜Ÿåˆ— âš¡
            res_frame_queue.put(combined_frame)
            if index % 10 == 0:  # æ¯10å¸§æ‰“å°ä¸€æ¬¡
                logger.info(f"âœ… Put frame {index} into queue (qsize={res_frame_queue.qsize()})")
            index += 1

    logger.info('Inference thread stopped')


class MuseTalkRealtimeEngine:
    """
    MuseTalk å®æ—¶æ¨ç†å¼•æ“ - ä½¿ç”¨ threading.Thread

    å®Œå…¨å‚è€ƒ virtual-tutor/lip-sync/musereal.py
    """

    def __init__(
        self,
        avatar_id: str,
        avatar_path: str,
        musetalk_base: str,
        batch_size: int = 8
    ):
        self.avatar_id = avatar_id
        self.avatar_path = avatar_path
        self.musetalk_base = musetalk_base
        self.batch_size = batch_size

        # é˜Ÿåˆ—ï¼ˆä½¿ç”¨ Queueï¼Œthreading å…¼å®¹ï¼‰
        # å¢å¤§å®¹é‡ä»¥é¿å…é˜»å¡ï¼ˆæ”¯æŒ50ä¸ªbatchï¼‰
        self.audio_feat_queue = Queue(maxsize=50)
        self.res_frame_queue = Queue(maxsize=batch_size * 20)

        # æ§åˆ¶äº‹ä»¶
        self.render_event = Event()

        # æ¨ç†çº¿ç¨‹
        self.inference_thread: Optional[Thread] = None

        # æ¨¡å‹ï¼ˆåœ¨ start() æ—¶åŠ è½½ï¼‰
        self.vae = None
        self.unet = None
        self.pe = None
        self.timesteps = None
        self.audio_processor = None

        # Avatar æ•°æ®
        self.coord_list_cycle = None
        self.frame_list_cycle = None
        self.mask_list_cycle = None
        self.mask_coords_list_cycle = None
        self.input_latent_list_cycle = None

        logger.info(f"[{avatar_id}] Realtime Engine initialized")

    def start(self):
        """å¯åŠ¨æ¨ç†çº¿ç¨‹"""
        if self.inference_thread is not None:
            logger.warning(f"[{self.avatar_id}] Engine already started")
            return

        logger.info(f"[{self.avatar_id}] Loading models in main thread...")
        logger.info(f"[{self.avatar_id}] musetalk_base={self.musetalk_base}")
        logger.info(f"[{self.avatar_id}] Current sys.path has {len(sys.path)} entries")

        # ç¡®ä¿ MuseTalk è·¯å¾„åœ¨ sys.path ä¸­
        if self.musetalk_base not in sys.path:
            sys.path.insert(0, self.musetalk_base)
            logger.info(f"[{self.avatar_id}] Added {self.musetalk_base} to sys.path")
        else:
            logger.info(f"[{self.avatar_id}] {self.musetalk_base} already in sys.path")

        # åˆ‡æ¢åˆ° MuseTalk ç›®å½•ï¼ˆæ¨¡å‹åŠ è½½éœ€è¦ï¼‰
        original_cwd = os.getcwd()
        logger.info(f"[{self.avatar_id}] Changing directory from {original_cwd} to {self.musetalk_base}")
        os.chdir(self.musetalk_base)

        try:
            logger.info(f"[{self.avatar_id}] Attempting to import musetalk.utils.utils...")

            # æ¸…é™¤å¯èƒ½çš„æ¨¡å—ç¼“å­˜ï¼Œå¹¶ä¸´æ—¶è°ƒæ•´ sys.path
            import importlib
            
            # ä¿å­˜åŸå§‹ sys.path
            original_sys_path = sys.path.copy()
            
            # æ¸…é™¤æ‰€æœ‰ musetalk ç›¸å…³çš„æ¨¡å—ç¼“å­˜
            mods_to_remove = [k for k in sys.modules.keys() if k == 'musetalk' or k.startswith('musetalk.')]
            for mod in mods_to_remove:
                del sys.modules[mod]
            
            # æŠŠ MuseTalk æ”¾åˆ° sys.path æœ€å‰é¢
            sys.path = [self.musetalk_base] + [p for p in sys.path if p != self.musetalk_base]
            
            logger.info(f"[{self.avatar_id}] sys.path[0] = {sys.path[0]}")

            from musetalk.utils.utils import load_all_model
            from musetalk.whisper.audio2feature import Audio2Feature
            logger.info(f"[{self.avatar_id}] Import successful!")

            # 1. åŠ è½½ MuseTalk æ¨¡å‹ï¼ˆåœ¨ä¸»è¿›ç¨‹ä¸­ï¼‰
            vae, unet, pe = load_all_model()

            # 2. åˆ›å»º audio processor
            audio_processor = Audio2Feature(
                whisper_model_type="tiny",
                model_path="tiny"
            )

            # è½¬æ¢ä¸º half ç²¾åº¦
            pe = pe.half()
            vae.vae = vae.vae.half()
            unet.model = unet.model.half()

            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            timesteps = torch.tensor([0], device=device)

            self.vae = vae
            self.unet = unet
            self.pe = pe
            self.timesteps = timesteps
            self.audio_processor = audio_processor

            logger.info(f"[{self.avatar_id}] Models loaded on {device}")

            # 3. åŠ è½½ Avatar æ•°æ®
            self._load_avatar_data()

            # 4. å¯åŠ¨æ¨ç†çº¿ç¨‹ï¼ˆä½¿ç”¨ Threadï¼Œä¸æ˜¯ Processï¼ï¼‰
            self.render_event.set()
            self.inference_thread = Thread(
                target=inference_loop,
                args=(
                    self.render_event,
                    self.batch_size,
                    self.input_latent_list_cycle,
                    self.coord_list_cycle,
                    self.frame_list_cycle,
                    self.mask_list_cycle,
                    self.mask_coords_list_cycle,
                    self.audio_feat_queue,
                    self.res_frame_queue,
                    self.vae, self.unet, self.pe, self.timesteps
                ),
                daemon=True
            )
            self.inference_thread.start()

            logger.info(f"[{self.avatar_id}] Inference thread started")

        finally:
            os.chdir(original_cwd)

    def _load_avatar_data(self):
        """åŠ è½½ Avatar æ•°æ®"""
        # åŠ è½½ latents
        latents_path = os.path.join(self.avatar_path, "latents.pt")
        if not os.path.exists(latents_path):
            raise FileNotFoundError(f"Latents not found: {latents_path}")
        self.input_latent_list_cycle = torch.load(latents_path)

        # åŠ è½½ coords
        coords_path = os.path.join(self.avatar_path, "coords.pkl")
        with open(coords_path, 'rb') as f:
            self.coord_list_cycle = pickle.load(f)

        # åŠ è½½åŸå§‹å›¾åƒ
        full_imgs_path = os.path.join(self.avatar_path, "full_imgs")
        img_list = sorted(
            Path(full_imgs_path).glob("*.[jpJP][pnPN]*[gG]"),
            key=lambda x: int(x.stem)
        )
        self.frame_list_cycle = [cv2.imread(str(p)) for p in img_list]

        # åŠ è½½ mask
        mask_path = os.path.join(self.avatar_path, "mask")
        mask_coords_path = os.path.join(self.avatar_path, "mask_coords.pkl")

        with open(mask_coords_path, 'rb') as f:
            self.mask_coords_list_cycle = pickle.load(f)

        mask_list = sorted(
            Path(mask_path).glob("*.[jpJP][pnPN]*[gG]"),
            key=lambda x: int(x.stem)
        )
        self.mask_list_cycle = [cv2.imread(str(p)) for p in mask_list]

        logger.info(f"[{self.avatar_id}] Avatar data loaded: {len(self.coord_list_cycle)} frames")

    async def generate_frames(
        self,
        audio_data: str,
        fps: int = 25
    ) -> AsyncIterator[np.ndarray]:
        """
        ç”Ÿæˆå¸§æµï¼ˆå¼‚æ­¥ï¼‰

        æµç¨‹ï¼š
        1. è§£ç å¹¶è½¬æ¢éŸ³é¢‘æ ¼å¼ï¼ˆMP3 â†’ WAVï¼‰
        2. æå– Whisper ç‰¹å¾
        3. æ”¾å…¥é˜Ÿåˆ—
        4. ä»å¸§é˜Ÿåˆ—å®æ—¶è¯»å–å¹¶yield
        """
        import base64
        import tempfile
        import subprocess
        import soundfile as sf

        # 1. è§£ç éŸ³é¢‘
        audio_bytes = base64.b64decode(audio_data)

        # æ£€æµ‹éŸ³é¢‘æ ¼å¼å¹¶è½¬æ¢ä¸º WAVï¼ˆMuseTalk éœ€è¦ WAV æ ¼å¼ï¼‰
        # å…ˆä¿å­˜åŸå§‹éŸ³é¢‘ï¼ˆå¯èƒ½æ˜¯ MP3 æˆ– WAVï¼‰
        with tempfile.NamedTemporaryFile(suffix='.tmp', delete=False) as f:
            f.write(audio_bytes)
            temp_audio_path = f.name

        # ä½¿ç”¨ ffmpeg è½¬æ¢ä¸º 16kHz mono WAVï¼ˆMuseTalk è¦æ±‚çš„æ ¼å¼ï¼‰
        audio_path = temp_audio_path.replace('.tmp', '.wav')
        try:
            cmd = [
                'ffmpeg', '-y', '-i', temp_audio_path,
                '-ar', '16000',  # 16kHz é‡‡æ ·ç‡
                '-ac', '1',      # å•å£°é“
                '-f', 'wav',     # WAV æ ¼å¼
                audio_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            if result.returncode != 0:
                logger.error(f"[{self.avatar_id}] FFmpeg conversion failed: {result.stderr}")
                raise RuntimeError(f"Audio conversion failed: {result.stderr}")
            logger.info(f"[{self.avatar_id}] âœ… Audio converted to WAV: {audio_path}")
        finally:
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(temp_audio_path)
            except:
                pass

        try:
            # âš ï¸ é‡è¦ï¼šåœ¨å¼€å§‹æ–°è¯·æ±‚å‰æ¸…ç©ºé˜Ÿåˆ—ï¼Œé¿å…æ®‹ç•™å¸§å½±å“
            self._clear_queues()
            
            # 2. æå– Whisper ç‰¹å¾
            logger.info(f"[{self.avatar_id}] Extracting audio features...")

            loop = asyncio.get_event_loop()

            def extract_features():
                whisper_feature = self.audio_processor.audio2feat(audio_path)
                # ä¸ try ä¿æŒä¸€è‡´ï¼šfps=50 â†’ fps/2=25fps
                # æ¯å¸§è§†é¢‘å¯¹åº” 2 ä¸ªéŸ³é¢‘ chunks (40ms)
                return self.audio_processor.feature2chunks(
                    feature_array=whisper_feature,
                    fps=fps / 2  # fps=50 â†’ 25fps
                )

            whisper_chunks = await loop.run_in_executor(None, extract_features)

            logger.info(f"[{self.avatar_id}] Extracted {len(whisper_chunks)} chunks")

            # 3. å°† whisper_chunks åˆ†æ‰¹æ”¾å…¥éŸ³é¢‘é˜Ÿåˆ—
            # æ¯ä¸ªæ‰¹æ¬¡åŒ…å« batch_size ä¸ª chunks
            batched_chunks = []
            for i in range(0, len(whisper_chunks), self.batch_size):
                batch = whisper_chunks[i:i + self.batch_size]
                if len(batch) == self.batch_size:  # åªå¤„ç†å®Œæ•´çš„æ‰¹æ¬¡
                    batched_chunks.append(batch)

            logger.info(f"[{self.avatar_id}] Created {len(batched_chunks)} batches")

            # 4. æ”¾å…¥éŸ³é¢‘é˜Ÿåˆ—ï¼ˆå¼‚æ­¥ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            for i, batch in enumerate(batched_chunks):
                await loop.run_in_executor(
                    None,
                    lambda b=batch: self.audio_feat_queue.put(b)
                )
                if i == 0:
                    logger.info(f"[{self.avatar_id}] âœ… First batch added to audio queue")

            logger.info(f"[{self.avatar_id}] All {len(batched_chunks)} batches added to audio queue")

            # 5. ä»å¸§é˜Ÿåˆ—è¯»å–å¹¶ yield
            total_frames = len(batched_chunks) * self.batch_size
            frame_count = 0
            first_frame_time = None
            
            logger.info(f"[{self.avatar_id}] ğŸ”„ Waiting for frames... (expected {total_frames} frames)")

            while frame_count < total_frames:
                try:
                    # æ·»åŠ è°ƒè¯•æ—¥å¿—
                    if frame_count == 0:
                        logger.info(f"[{self.avatar_id}] ğŸ” Queue status before first get: qsize={self.res_frame_queue.qsize()}")
                    
                    frame = await loop.run_in_executor(
                        None,
                        lambda: self.res_frame_queue.get(timeout=2)
                    )

                    if frame_count == 0:
                        first_frame_time = time.time()
                        logger.info(f"[{self.avatar_id}] âš¡ First frame generated!")

                    yield frame
                    frame_count += 1
                    
                    if frame_count % 10 == 0:
                        logger.info(f"[{self.avatar_id}] ğŸ“¤ Yielded {frame_count}/{total_frames} frames")

                except Empty:
                    logger.warning(f"[{self.avatar_id}] â³ Queue timeout, retry... (frame_count={frame_count}, qsize={self.res_frame_queue.qsize()})")
                    await asyncio.sleep(0.01)
                    continue

            if first_frame_time:
                total_time = time.time() - first_frame_time
                logger.info(
                    f"[{self.avatar_id}] Generated {frame_count} frames "
                    f"in {total_time:.2f}s (avg {frame_count/total_time:.2f} fps)"
                )

        finally:
            os.unlink(audio_path)

    def _clear_queues(self):
        """æ¸…ç©ºéŸ³é¢‘å’Œå¸§é˜Ÿåˆ—ï¼Œé¿å…æ®‹ç•™æ•°æ®å½±å“æ–°è¯·æ±‚"""
        # æ¸…ç©ºéŸ³é¢‘ç‰¹å¾é˜Ÿåˆ—
        cleared_audio = 0
        while not self.audio_feat_queue.empty():
            try:
                self.audio_feat_queue.get_nowait()
                cleared_audio += 1
            except Empty:
                break
        
        # æ¸…ç©ºå¸§é˜Ÿåˆ—
        cleared_frames = 0
        while not self.res_frame_queue.empty():
            try:
                self.res_frame_queue.get_nowait()
                cleared_frames += 1
            except Empty:
                break
        
        if cleared_audio > 0 or cleared_frames > 0:
            logger.info(f"[{self.avatar_id}] ğŸ§¹ Cleared queues: {cleared_audio} audio batches, {cleared_frames} frames")

    def stop(self):
        """åœæ­¢æ¨ç†çº¿ç¨‹"""
        self.render_event.clear()
        if self.inference_thread:
            self.inference_thread.join(timeout=5)
        logger.info(f"[{self.avatar_id}] Engine stopped")
